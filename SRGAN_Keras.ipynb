{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentinel_Cartosat_SR_SRGAN_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeWkwcccqrM"
      },
      "source": [
        "#################### Super Resolution of Images ######################################### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm70uDVxfN6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "05efc7c5-a71d-4d32-de70-dbcbcf53f870"
      },
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from skimage.io import imsave\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input, Dense , Add , Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsCOARyjgVlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "75c94996-f8ac-4a22-d6a3-999de2ba0955"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-pX3cK3MzAV"
      },
      "source": [
        "# Loading the training images and preprocessing Step\n",
        "\n",
        "def splitImg(image,dim):\n",
        "    ''' Takes input image and splits it into multiple images of dimension dim\n",
        "    Outputs an array consisting of smaller images in order '''\n",
        "\n",
        "    X=image.shape[0]/dim[0]\n",
        "    Y=image.shape[1]/dim[1]\n",
        "    no_images=X*Y\n",
        "    #print(\"No. of images:\",no_images)\n",
        "    #print(\"Shape of image:\",image.shape)\n",
        "    images=[]\n",
        "    i=0;j=0\n",
        "    count=0\n",
        "    while i <image.shape[0]:\n",
        "        j=0\n",
        "        while j<image.shape[1]:\n",
        "            img=image[i:i+dim[0],j:j+dim[1],:]\n",
        "            images.append(img)\n",
        "            #count=count+1\n",
        "            #print(\"Count: \",count)\n",
        "            j=j+dim[1]\n",
        "        i=i+dim[0]    \n",
        "    return(np.array(images) , no_images)\n",
        "\n",
        "\n",
        "def dataPreProcessing(dimHR,dimLR):\n",
        "  ''' Takes dim as input which is the dimension of required training images to be obtained from the satellite image\n",
        "      Outputs two arrays of high_resolution and low_resloution images\n",
        "  '''\n",
        "  # path='pathaddress'\n",
        "  path = '/content/drive/My Drive/Super_Resolution/SRGAN/'\n",
        "\n",
        "  images = glob.glob( os.path.join(path,'Train_Images', '*.tif'))\n",
        "\n",
        "  low_resolution_image=cv2.imread(path+'Train_Images/LR1000.tif')\n",
        "  high_resolution_image=cv2.imread(path+'Train_Images/HR2000.tif')\n",
        "\n",
        "  print('Dimension of HR Image: ',high_resolution_image.shape)\n",
        "  print('Dimension of LR Image: ',low_resolution_image.shape)\n",
        "\n",
        "  train_hr , no_images = splitImg(high_resolution_image,dimHR)\n",
        "  train_lr , no_images= splitImg(low_resolution_image,dimLR)\n",
        "\n",
        "  print(\"Total No. of Images: \",int(no_images))\n",
        "\n",
        "  train_hr=train_hr.astype('float64')\n",
        "  train_lr=train_lr.astype('float64')\n",
        "\n",
        "  return(train_hr , train_lr , int(no_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tdFVnOTawc3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abiSutH_Mz35"
      },
      "source": [
        "# Function to randomly sample images from trainig set\n",
        "\n",
        "def sampleImages(batch_size,train_hr,train_lr,no_images):\n",
        "  # Generates a random batch of size batch_size \n",
        " \n",
        " \n",
        "  random_batch = np.random.choice(no_images, batch_size , replace=False)\n",
        "\n",
        "  high_resolution_images = np.array([train_hr[i] for i in random_batch])\n",
        "  low_resolution_images = np.array([train_lr[i] for i in random_batch])\n",
        "\n",
        "  return(high_resolution_images , low_resolution_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E15K6TnbWxTJ"
      },
      "source": [
        "def stichImages(images,dim):\n",
        "  '''Takes images array and dimension to output image as input\n",
        "  Outputs a stiched image of dimension dim '''\n",
        "\n",
        "\n",
        "  out=np.zeros((dim[0],dim[1],3))\n",
        "  X=images[0].shape[0];Y=images[0].shape[1]\n",
        "    \n",
        "  (i,j,k)=(0,0,0)\n",
        "  while i < dim[0]:\n",
        "      j=0\n",
        "      while j < dim[1]:\n",
        "          out[i:i+X,j:j+Y,:] = images[k]\n",
        "          k=k+1\n",
        "          j=j+Y\n",
        "      i=i+X\n",
        "  return(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efMyjG1oaLLl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt7ELGDEWx4K"
      },
      "source": [
        "def save_images(low_resolution_image, original_image, generated_image,path):\n",
        "    \"\"\"\n",
        "    Save low-resolution, high-resolution(original) and\n",
        "    generated high-resolution images in a single image\n",
        "    \"\"\"\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 3, 1)\n",
        "    ax.imshow(low_resolution_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"LR\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 2)\n",
        "    ax.imshow(original_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Original HR\")\n",
        "\n",
        "    ax = fig.add_subplot(1, 3, 3)\n",
        "    ax.imshow(generated_image)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Generated SR\")\n",
        "\n",
        "    plt.savefig(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E9NlqaddeOL"
      },
      "source": [
        "def residual_block(x):\n",
        "  \"\"\"\n",
        "  Residual block\n",
        "  \"\"\"\n",
        "  \n",
        "  filters = [64, 64]\n",
        "  kernel_size = 3\n",
        "  strides = 1\n",
        "  padding = \"same\"\n",
        "  momentum = 0.8\n",
        "  activation = \"relu\"\n",
        "\n",
        "  res = Conv2D(filters=filters[0], kernel_size=kernel_size,\n",
        "  strides=strides, padding=padding)(x)\n",
        "  res = Activation(activation=activation)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "  res = Conv2D(filters=filters[1], kernel_size=kernel_size,\n",
        "  strides=strides, padding=padding)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "\n",
        "  # Add res and x\n",
        "  res = Add()([res, x])\n",
        "  \n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI1fb55Ydq8E"
      },
      "source": [
        "\n",
        "def upsample(x, scale, num_filters):\n",
        "  def upsample_1(x, factor, **kwargs):\n",
        "      \"\"\"Sub-pixel convolution.\"\"\"\n",
        "      x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
        "      return Lambda(pixel_shuffle(scale=factor))(x)\n",
        "\n",
        "  if scale == 2:\n",
        "      x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
        "  elif scale == 3:\n",
        "      x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
        "  elif scale == 4:\n",
        "      x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
        "      x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
        "\n",
        "def pixel_shuffle(scale):\n",
        "  return lambda x: tensorflow.nn.depth_to_space(x, scale)\n",
        "\n",
        "def build_generator():\n",
        "  \"\"\"\n",
        "  Create a generator network using the hyperparameter values defined below\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  residual_blocks = 16\n",
        "  momentum = 0.8\n",
        "  input_shape = (100, 100, 3)\n",
        "\n",
        "  # Input Layer of the generator network\n",
        "  input_layer = Input(shape=input_shape)\n",
        "\n",
        "  # Add the pre-residual block\n",
        "  gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "  # Add 16 residual blocks\n",
        "  res = residual_block(gen1)\n",
        "  for i in range(residual_blocks - 1):\n",
        "      res = residual_block(res)\n",
        "\n",
        "  # Add the post-residual block\n",
        "  gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n",
        "  gen2 = BatchNormalization(momentum=momentum)(gen2)\n",
        "\n",
        "  # Take the sum of the output from the pre-residual block(gen1) and the post-residual block(gen2)\n",
        "  gen3 = Add()([gen2, gen1])\n",
        "\n",
        "  # Add an upsampling block\n",
        "  gen4=Conv2D(64 * (2 ** 2), 3, padding='same')(gen3)\n",
        "  gen4 = tensorflow.nn.depth_to_space(gen4, 2)\n",
        "  #gen4 = upsample(gen3, scale = 2, num_filters = 64)\n",
        "  gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n",
        "  gen4 = Activation('relu')(gen4)\n",
        "\n",
        "  # Output convolution layer\n",
        "  gen5 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen4)\n",
        "  output = Activation('tanh')(gen5)\n",
        "\n",
        "  # Keras model\n",
        "  model = Model(inputs=[input_layer], outputs=[output], name='generator')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IJbHQCYex0l"
      },
      "source": [
        "def build_discriminator():\n",
        "    \"\"\"\n",
        "    Create a discriminator network using the hyperparameter values defined below\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    leakyrelu_alpha = 0.2\n",
        "    momentum = 0.8\n",
        "    input_shape = (200, 200, 3)\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Add the first convolution block\n",
        "    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n",
        "    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "    # Add the 2nd convolution block\n",
        "    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n",
        "    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        "    dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "\n",
        "    # Add the third convolution block\n",
        "    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n",
        "    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        "    dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "\n",
        "    # Add the fourth convolution block\n",
        "    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n",
        "    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        "    dis4 = BatchNormalization(momentum=0.8)(dis4)\n",
        "\n",
        "    # Add the fifth convolution block\n",
        "    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n",
        "    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        "    dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "\n",
        "    # Add the sixth convolution block\n",
        "    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n",
        "    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        "    dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "\n",
        "\n",
        "    # Add a dense layer\n",
        "    dis7 = Dense(units=1024)(dis6)\n",
        "    dis7 = LeakyReLU(alpha=0.2)(dis7)\n",
        "\n",
        "    # Last dense layer - for classification\n",
        "    output = Dense(units=1, activation='sigmoid')(dis7)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_hXP-l7jkMm"
      },
      "source": [
        "def build_vgg():\n",
        "    \"\"\"\n",
        "    Build VGG network to extract image features\n",
        "    \"\"\"\n",
        "    input_shape = (200, 200, 3)\n",
        "\n",
        "    # Load a pre-trained VGG19 model trained on 'Imagenet' dataset\n",
        "    vgg = VGG19(weights=\"imagenet\",include_top=False)\n",
        "    vgg.outputs = [vgg.layers[9].output]\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Extract features\n",
        "    features = vgg(input_layer)\n",
        "\n",
        "    # Create a Keras model\n",
        "    model = Model(inputs=[input_layer], outputs=[features])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aItGvwX9BD1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyEFbZ0sjkuy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "22e17108-9016-455c-b1f5-776145c1902e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    path='/content/drive/My Drive/Writing/Super_Resolution/SRGAN/Train_Images/'\n",
        "    epochs = 300\n",
        "    batch_size = 1\n",
        "    mode = 'train'\n",
        "\n",
        "    # Shape of low-resolution and high-resolution images\n",
        "    low_resolution_shape = (100,100, 3)\n",
        "    high_resolution_shape = (200, 200, 3)\n",
        "\n",
        "    # Common optimizer for all networks\n",
        "    common_optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "    if mode == 'train':\n",
        "        # Build and compile VGG19 network to extract features\n",
        "        vgg = build_vgg()\n",
        "        vgg.trainable = False\n",
        "        vgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the discriminator network\n",
        "        discriminator = build_discriminator()\n",
        "        discriminator.compile(loss='binary_crossentropy', optimizer=common_optimizer, metrics=['accuracy'])\n",
        "\n",
        "        #print(discriminator.metrics_names)\n",
        "\n",
        "        # Build the generator network\n",
        "        generator = build_generator()\n",
        "\n",
        "        \"\"\"\n",
        "        Build and compile the adversarial model\n",
        "        \"\"\"\n",
        "\n",
        "        # Input layers for high-resolution and low-resolution images\n",
        "        input_high_resolution = Input(shape=high_resolution_shape)\n",
        "        input_low_resolution = Input(shape=low_resolution_shape)\n",
        "\n",
        "        # Generate high-resolution images from low-resolution images\n",
        "        generated_high_resolution_images = generator(input_low_resolution)\n",
        "\n",
        "        # Extract feature maps of the generated images\n",
        "        features = vgg(generated_high_resolution_images)\n",
        "\n",
        "        # Make the discriminator network as non-trainable\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # Get the probability of generated high-resolution images\n",
        "        probs = discriminator(generated_high_resolution_images)\n",
        "\n",
        "        # Create and compile an adversarial model\n",
        "        adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n",
        "        adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n",
        "\n",
        "        #print(adversarial_model.metrics_names)\n",
        "\n",
        "        # Getting the training images\n",
        "\n",
        "        train_hr , train_lr , no_images = dataPreProcessing((200,200),(100,100))\n",
        "\n",
        "        # No. of iterations for an epoch\n",
        "        noIter=int(no_images/batch_size)\n",
        "\n",
        "        gen_loss=[]\n",
        "        dis_loss=[]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Epoch:{}\".format(epoch))\n",
        "\n",
        "            \"\"\"\n",
        "            Train the discriminator network\n",
        "            \"\"\"\n",
        "            gen=0\n",
        "            dis=0\n",
        "            for i in range(noIter):\n",
        "              print(\"Epoch:{} Iteration:{}\".format(epoch,i+1))\n",
        "\n",
        "            \n",
        "              # Sample a batch of images\n",
        "              high_resolution_images , low_resolution_images = sampleImages(batch_size,train_hr,train_lr,no_images)\n",
        "              \n",
        "              \n",
        "              #print(\"High Images: \",high_resolution_images)\n",
        "              \n",
        "              # Normalize images\n",
        "              high_resolution_images = high_resolution_images / 255\n",
        "              low_resolution_images = low_resolution_images / 255\n",
        "\n",
        "              # Generate high-resolution images from low-resolution images\n",
        "              generated_high_resolution_images = generator.predict(low_resolution_images)\n",
        "\n",
        "              # Generate batch of real and fake labels\n",
        "              real_labels = np.ones((batch_size, 25, 25, 1))\n",
        "              fake_labels = np.zeros((batch_size, 25, 25, 1))\n",
        "\n",
        "              # Make disciminaotr trainable\n",
        "              discriminator.trainable = True\n",
        "\n",
        "              # Train the discriminator network on real and fake images\n",
        "              d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n",
        "              d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n",
        "\n",
        "              \n",
        "              # Calculate total discriminator loss\n",
        "              d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "              print(\"d_loss:\", d_loss)\n",
        "              dis+=d_loss[0]\n",
        "\n",
        "              \"\"\"\n",
        "              Train the generator network\n",
        "              \"\"\"\n",
        "\n",
        "              # Make disciminaotr non-trainable\n",
        "              discriminator.trainable = False\n",
        "\n",
        "              # Extract feature maps for real high-resolution images\n",
        "              image_features = vgg.predict(high_resolution_images)\n",
        "\n",
        "              # Train the generator network\n",
        "              g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images],\n",
        "                                              [real_labels, image_features])\n",
        "\n",
        "              print(\"g_loss:\", g_loss)\n",
        "              gen+=g_loss[0]\n",
        "\n",
        "            gen_loss.append(gen/noIter)\n",
        "            dis_loss.append(dis/noIter)\n",
        "\n",
        "            if epoch%10 == 0:\n",
        "              generated=[]\n",
        "\n",
        "              for img in train_lr:\n",
        "                batch_lr=np.array([img])/255\n",
        "\n",
        "                generated_image = generator.predict_on_batch(batch_lr)\n",
        "                generated.append(generated_image[0])\n",
        "              generated=np.array(generated)\n",
        "\n",
        "              stiched_hr=stichImages(train_hr,(2000,2000))\n",
        "              stiched_lr=stichImages(train_lr, (1000,1000))\n",
        "              stiched_generated=stichImages(generated,(2000,2000))\n",
        "\n",
        "              savepath=os.path.join(path,'Output_Images1')\n",
        "              #save_path=os.path.join(path,'/ResultImages')\n",
        "              save_images(stiched_lr/255, stiched_hr/255, stiched_generated, path=os.path.join(savepath,'Epoch-{}'.format(epoch+1)))\n",
        "              #print(\"kj\")\n",
        "              imsave(os.path.join(savepath,'Epoch-{}_FullImage.jpg'.format(epoch+1)),stiched_generated)\n",
        "              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d681ce08377e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Getting the training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_hr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_lr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mno_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPreProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# No. of iterations for an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e5bfd1be7538>\u001b[0m in \u001b[0;36mdataPreProcessing\u001b[0;34m(dimHR, dimLR)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mhigh_resolution_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Train_Images/HR2000.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dimension of HR Cartosat Image: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhigh_resolution_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dimension of LR Sentinel Image: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_resolution_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mH8_gGbjlGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "9e7a26d0-0e6c-4fb5-e217-d224484e58f7"
      },
      "source": [
        "modelpath=os.path.join(path,'SavedModels')\n",
        "generator.save(os.path.join(modelpath,'Generator.h5'))\n",
        "discriminator.save(os.path.join(modelpath,'Discriminator.h5'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8fbd13778596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SavedModels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Generator.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Discriminator.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1171\u001b[0;31m                       signatures)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m    107\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    108\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 109\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/drive/My Drive/tiff_data4/SavedModels/Generator.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OwgzMNgj1VN"
      },
      "source": [
        "epoch_array=[i for i in range(epochs)]\n",
        "plot = plt.plot(epoch_array , dis_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Generator Loss')\n",
        "plt.title(\"SRGAN Generator Loss VS Epochs\")\n",
        "plt.savefig(os.path.join(path , 'SRGAN_Gen_Loss_Epoch.jpg'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx7xXC5p4o_u"
      },
      "source": [
        "plot = plt.plot(epoch_array , gen_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Discriminator Loss')\n",
        "plt.title(\"SRGAN Discriminator Loss VS Epochs\")\n",
        "plt.savefig(os.path.join(path , 'SRGAN_Dis_Loss_Epoch.jpg'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gI1N6VG8Z38"
      },
      "source": [
        "testImage=cv2.imread(os.path.join(path , 'Test.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEmSE3JUL4Uj"
      },
      "source": [
        "test_lr , no_images= splitImg(testImage,(100,100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V5lt89TL7j4"
      },
      "source": [
        "generated=[]\n",
        "\n",
        "for img in test_lr:\n",
        "  batch_lr=np.array([img])/255\n",
        "\n",
        "  generated_image = generator.predict_on_batch(batch_lr)\n",
        "  generated.append(generated_image[0])\n",
        "generated=np.array(generated)\n",
        "\n",
        "stiched_generated=stichImages(generated,(2000,2000))\n",
        "stiched_generated = 255 *stiched_generated # Now scale by 255\n",
        "stiched_generated = stiched_generated.astype(np.uint8)\n",
        "\n",
        "imsave(os.path.join(path,'Test_FullImage.jpg'),stiched_generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ltnkl9EMbQI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
